import torch
from torch.utils.data import Dataset, DataLoader, random_split
from stdatasets.datasets import PseudoDataset, StDataset
from models.gmgat_model import GMGATModel
from utils.lossFunctions import LossFunctions
import torch.nn.functional as F
import argparse
import json
import wandb
import os
from tqdm import tqdm
import random
import numpy as np
from generate_spots import pseudo_spot_generation
import scanpy as sc
from utils.makeGraph import ST_preprocess
import warnings
import time
warnings.filterwarnings("ignore")

parser = argparse.ArgumentParser(description='Process JSON input')
parser.add_argument('--input', '-i', type=str, help='Path to the JSON file')
args = parser.parse_args()

with open(args.input) as file:
    config = json.load(file)

default_hyper = {
    'gpu_id': None, # Int: The number of chosen gpu. If None, CPU will be deployed.
    'proj_name': None, # Str: For wandb recording. If None, the training log will not be stored.
    'sub_name': None, # Str: For naming the output documents, and subtitle of the wandb project.
    'real_st_path': None, # Str: The path to gene expression profile of ST, ending with .h5ad. See data preparation.
    'real_location_path': None, # Str: The path to X-Y location of ST with specified format. See data preparation.
    'pseudo_st_path': None, # Str(optional): The path to pseudo spots generated by scRNA-seqs, ending with .h5ad.
    # If None, the model will synthesize the spots from scratch. In that case, sc_path is mandatory.
    'num_spots': 50000, # Int(optional): The number of synthesized pseudo spots. Make effect when pseudo_st_path is None.
    'sc_path': None, # Str(optional): The path to single cell RNA sequencing, ending with .h5ad. See data preparation.
    'mean_num_cells_each_spot': 6, # Int(optional): The expected mean number of cells within each spot.
    # Make effect when pseudo_st_path is None.
    'max_cell_types_each_spot': 4, # Int(optional): The expected maximum number of cell types within each spot.
    # Make effect when pseudo_st_path is None.
    'num_cpu': 4, # Int(optional): The number of CPU deployed for pseudo spots generation.
    'num_nodes': 200, # Int(optional): The number of nodes within each pseudo graph,
    # In contrast with the number of spots in the ST.
    'num_epochs': 20, # Int(optional): The number of training epochs. Default set to 20.
    'seed_list': [1], # List(optional): The list of random seed, for investigation, confirmation and reproduction.
    'hvg': True, # Bool(optional): Whether to select highly variable genes, suggest to True.
    'scale': True, # Bool(optional): Whether to scale the gene count values, suggest to True.
    'domain_classes': 16, # Int(optional): The number of hidden mixtures. Default set to 16.
    'entropy_ratio': 10.0, # Float(optional): The weight of entropy loss. Default set to 10.0.
    # Intuitively, the higher value produces the purer results.
    'lr': 0.0001, # Float(optional): Learning rate applied for training. Default set to 0.0001.
    'marker_path': None, # Str(optional): The path to marker gene list with specified format. See data preparation.
    'spatial_dist': 1.5, # Float(optional): The distance threshold for finding the nearest neighbors.
    # Default set to 1.5, equivalent to k=6 in KNN.
    'sample_ratio': 1.0, # Float(optional): The weight of sample loss. Default set to 1.0.
    'early_stop': False, # Bool(optional): Whether to deploy early stop strategy. Default set to False.
    'interval': 2, # Int(optional): The interval of saving model (default every two epochs).
    'adaptation_ratio': 1.0, # Float(optional): The weight of domain adaptation loss. Default set to 1.0.
    "result": True, # Bool(optional): Whether to output deconvolution results.
    "save_spot": False
}

# Update default hyperparameters with values from the JSON file
hyperparameters = {key: config.get(key, default_hyper[key]) for key in default_hyper}

gpu_id = hyperparameters['gpu_id']
proj_name = hyperparameters['proj_name']
sub_name = hyperparameters['sub_name']
real_st_path = hyperparameters['real_st_path']
real_location_path = hyperparameters['real_location_path']
pseudo_st_path = hyperparameters['pseudo_st_path']
num_spots = hyperparameters['num_spots']
sc_path = hyperparameters['sc_path']
mean_num_cells_each_spot = hyperparameters['mean_num_cells_each_spot']
max_cell_types_each_spot = hyperparameters['max_cell_types_each_spot']
num_cpu = hyperparameters['num_cpu']
pseudo_node_num = hyperparameters['num_nodes']
num_epochs = hyperparameters['num_epochs']
seed_list = hyperparameters['seed_list']
hvg = hyperparameters['hvg']
scale = hyperparameters['scale']
domain_classes = hyperparameters['domain_classes']
entropy_ratio = hyperparameters['entropy_ratio']
lr = hyperparameters['lr']
marker_path = hyperparameters['marker_path']
spatial_dist = hyperparameters['spatial_dist']
global_ratio = hyperparameters['sample_ratio']
early_stop = hyperparameters['early_stop']
interval = hyperparameters['interval']
adaptation_ratio = hyperparameters['adaptation_ratio']
result = hyperparameters['result']
save_spot = hyperparameters['save_spot']

if early_stop:
    num_spots = int(num_spots / 0.8)

def train(model, pseudo_loader, real_loader, loss_functions, gpu_id, proj_name, sub_name, num_epochs, seed):
    if proj_name:
        wandb.init(project=proj_name, name=f"{sub_name}_{str(seed)}")

    if gpu_id != None:
        device = torch.device("cuda:{}".format(gpu_id))
        print(f"Using GPU: {gpu_id}")
    else:
        device = torch.device("cpu")
        print(f"Using CPU")

    if early_stop:
        pseudo_loader.seed = seed

        window_size = 3

        no_improvement_count = 0

        best_loss  = np.inf

        train_ratio = 0.8
        val_ratio = 1 - train_ratio

        # Calculate the number of samples for training and validation
        dataset_size = len(pseudo_loader.dataset)
        train_size = int(train_ratio * dataset_size)
        val_size = dataset_size - train_size

        # Split the DataLoader into training and validation DataLoader
        train_set, val_set = random_split(pseudo_loader.dataset, [train_size, val_size])

        # Create new DataLoaders for training and validation
        pseudo_loader = DataLoader(train_set, batch_size=1, shuffle=True)
        val_loader = DataLoader(val_set, batch_size=1, shuffle=False)

        print(f"training set:{train_size}, validation set:{val_size}")

    model.to(device)
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    path = f"./checkpoints/{sub_name}"
    if not os.path.exists(path):
        os.makedirs(path)
        print("Directory created:", path)
    else:
        print("Directory already exists:", path)
    for epoch in range(num_epochs):
        running_recon_loss = 0.0
        running_mse_loss = 0.0
        running_ce_loss = 0.0
        running_global_loss = 0.0
        running_self_loss = 0.0
        with tqdm(pseudo_loader, unit="batch") as tepoch:
            for x, y, ex_adj in tepoch:
                tepoch.set_description(f"Epoch {epoch + 1}")
                x, y, ex_adj = x.to(device), y.to(device), ex_adj.to(device)
                ex_adj = torch.squeeze(ex_adj)
                ex_adj = ex_adj.fill_diagonal_(1.)
                optimizer.zero_grad()

                feature_dict, cls, ratio = model(x, ex_adj, mode="sc")



                zeros = torch.zeros((cls.shape[1])).type(torch.LongTensor).to(device)
                recon_loss = loss_functions.reconstruction_graph_loss(ex_adj, feature_dict['reconstruct_graph'])
                # if global_loss:
                mse_los = 1000 * F.mse_loss(ratio[1], y)
                # else:
                #     mse_los = 1000 * F.mse_loss(ratio, y)

                ## ce_loss should be invested deeper
                ce_loss = loss_functions.ce_loss(cls.squeeze(), zeros)

                if epoch < 3:
                    loss = mse_los
                else:
                    loss = recon_loss + mse_los + ce_loss * adaptation_ratio

                global_los = 1000 * F.mse_loss(ratio[1].mean(axis=1), y.mean(axis=1)) * global_ratio
                self_los = entropy_ratio * (
                            loss_functions.self_entropy(ratio[1].mean(axis=1)) + loss_functions.self_entropy(ratio[1]))

                if epoch >= 3:
                    loss = loss + global_los + self_los
                running_global_loss += global_los.item()
                running_self_loss += self_los.item()

                loss.backward()
                optimizer.step()

                running_recon_loss += recon_loss.item()
                running_mse_loss += mse_los.item()
                running_ce_loss += ce_loss.item()

        average_sc_mse_loss = running_mse_loss / len(pseudo_loader)
        average_sc_ce_loss = running_ce_loss / len(pseudo_loader)
        average_sc_recon_loss = running_recon_loss / len(pseudo_loader)
        average_sc_global_loss = running_global_loss / len(pseudo_loader)
        average_sc_self_loss = running_self_loss / len(pseudo_loader)

        running_recon_loss = 0.0
        running_ce_loss = 0.0


        with tqdm(real_loader, unit="batch") as repoch:
            for x, ex_adj, sp_adj in repoch:
                repoch.set_description(f"Epoch {epoch + 1}")
                x, ex_adj, sp_adj = x.to(device), ex_adj.to(device), sp_adj.to(device)
                ex_adj, sp_adj = torch.squeeze(ex_adj), torch.squeeze(sp_adj)
                ex_adj = ex_adj.fill_diagonal_(1.)
                sp_adj = sp_adj.fill_diagonal_(1.)
                optimizer.zero_grad()

                feature_dict, cls, ratio = model(x, [ex_adj, sp_adj], mode="st")

                ones = torch.ones((cls[0].shape[1])).type(torch.LongTensor).to(device)
                recon_loss_ex = loss_functions.reconstruction_graph_loss(ex_adj, feature_dict[0]['reconstruct_graph'])
                recon_loss_sp = loss_functions.reconstruction_graph_loss(sp_adj, feature_dict[1]['reconstruct_graph'])
                # js_loss = loss_functions.js_loss(ratio[0], ratio[1])
                ce_loss_ex = loss_functions.ce_loss(cls[0].squeeze(), ones)
                ce_loss_sp = loss_functions.ce_loss(cls[1].squeeze(), ones)

                recon_loss = recon_loss_ex + recon_loss_sp

                ce_loss = ce_loss_ex + ce_loss_sp

                running_recon_loss += recon_loss.item()
                # running_js_loss += js_loss.item()
                running_ce_loss += ce_loss.item()

                loss = recon_loss + ce_loss * adaptation_ratio

                loss.backward()
                optimizer.step()

                # running_recon_loss += recon_loss.item()
                # running_js_loss += js_loss.item()
                # running_ce_loss += ce_loss.item()
        average_st_recon_loss = running_recon_loss / len(real_loader)
        average_st_ce_loss = running_ce_loss / len(real_loader)

        if proj_name:
            wandb.log({'epoch': epoch+ 1, 'average_sc_recon_loss': average_sc_recon_loss })
            wandb.log({'epoch': epoch + 1, 'average_sc_mse_loss': average_sc_mse_loss})
            wandb.log({'epoch': epoch + 1, 'average_sc_ce_loss': average_sc_ce_loss})
            wandb.log({'epoch': epoch + 1, 'average_st_recon_loss': average_st_recon_loss})
            wandb.log({'epoch': epoch + 1, 'average_st_ce_loss': average_st_ce_loss})
            wandb.log({'epoch': epoch + 1, 'average_st_global_loss': average_sc_global_loss})
            wandb.log({'epoch': epoch + 1, 'average_st_self_loss': average_sc_self_loss})

        print('average_mse_loss: %.3f' % (average_sc_mse_loss))

        # print('average_sc_recon_loss: %.3f, average_sc_mse_loss: %.3f' % (average_sc_recon_loss, average_sc_mse_loss))
        # print('average_sc_ce_loss: %.3f, average_st_recon_loss: %.3f' % (average_sc_ce_loss, average_st_recon_loss))
        # print('average_st_ce_loss: %.3f' % average_st_ce_loss)
        # print('average_sc_global_loss: %.3f, average_sc_self_loss: %.3f' % (average_sc_global_loss, average_sc_self_loss))

        if early_stop:
            running_val_mse_los = 0
            with torch.no_grad():
                with tqdm(val_loader, unit="batch") as tepoch:
                    for x, y, ex_adj in tepoch:
                        tepoch.set_description(f"Epoch {epoch + 1}")
                        x, y, ex_adj = x.to(device), y.to(device), ex_adj.to(device)
                        ex_adj = torch.squeeze(ex_adj)
                        ex_adj = ex_adj.fill_diagonal_(1.)

                        feature_dict, cls, ratio = model(x, ex_adj, mode="sc")

                        val_mse_los = 1000 * F.mse_loss(ratio[1], y)

                        running_val_mse_los += val_mse_los.item()
            validation_loss = running_val_mse_los / len(val_loader)

            if proj_name:
                wandb.log({'epoch': epoch + 1, 'validation_loss': validation_loss})
            print('validation_loss: %.3f' % validation_loss)

            if epoch >= 3:
                if validation_loss < best_loss:
                    best_loss = validation_loss
                    no_improvement_count = 0
                else:
                    no_improvement_count += 1

            if no_improvement_count >= window_size:
                print("Early stopping triggered!")
                model_name = f"{sub_name}_{epoch + 1}_seed{seed}_early_stop.model"
                save_path = os.path.join(path, model_name)
                torch.save(model.state_dict(), save_path)
                print(f"save model at {model_name}")
                break


        if (epoch+1) % interval == 0 and (epoch+1) > 10:
            model_name = f"{sub_name}_{epoch+1}_seed{seed}.model"
            save_path = os.path.join(path, model_name)
            torch.save(model.state_dict(), save_path)
            print(f"save model at {model_name}")

    if proj_name:
        wandb.finish()

    print('Finished Training')

def read_and_process_data(sc_path, real_st_path,num_spots,mean_num_cells_each_spot,max_cell_types_each_spot,num_cpu,save_spot):
    sc_adata = sc.read_h5ad(sc_path)
    st_adata = sc.read_h5ad(real_st_path)

    start_time = time.time()

    if hvg:
        hvg_adata = ST_preprocess(st_adata, highly_variable_genes=hvg)
        st_genes = hvg_adata.var_names
    else:
        st_genes = st_adata.var_names
    sc_genes = sc_adata.var.index.values
    common_genes = set(st_genes).intersection(set(sc_genes))

    sc_adata_filter = sc_adata[:, list(common_genes)]

    spots_adata = pseudo_spot_generation(sc_exp=sc_adata_filter, spot_num=num_spots, lam=mean_num_cells_each_spot - 1,
                                         generation_method='celltype',
                                         max_cell_types_in_spot=max_cell_types_each_spot, n_jobs=num_cpu)
    if save_spot:
        spots_adata.write('./pseudo_spots_tmp.h5ad')
        pseudo_st_path = './pseudo_spots_tmp.h5ad'

        return pseudo_st_path
    else:
        return spots_adata

if __name__ == '__main__':
    start_time = time.time()
    if not pseudo_st_path:
        if save_spot:
            pseudo_st_path = read_and_process_data(sc_path, real_st_path,num_spots,mean_num_cells_each_spot,max_cell_types_each_spot,num_cpu,save_spot)
            pseudo_spots_adata = sc.read_h5ad(pseudo_st_path)
        else:
            pseudo_spots_adata = read_and_process_data(sc_path, real_st_path, num_spots, mean_num_cells_each_spot,
                                                   max_cell_types_each_spot, num_cpu, save_spot)
            pseudo_st_path = pseudo_spots_adata
        import gc
        # Force garbage collection
        gc.collect()

    real_dataset = StDataset(data_path=real_st_path, location_path=real_location_path, pseudo_st_path=pseudo_st_path,
                             hvg=hvg, scale=scale, marker_path=marker_path, spatial_dist=spatial_dist, k=int(6*spatial_dist/1.5))

    if isinstance(pseudo_st_path, str):
        pseudo_spots_adata = sc.read_h5ad(pseudo_st_path)
    else:
        pseudo_spots_adata = pseudo_st_path

    num_cell_types = len(pseudo_spots_adata.obs.columns) - 1

    header = pseudo_spots_adata.obs.columns[:-1]

    del(pseudo_spots_adata)


    num_hvg = len(real_dataset[0][0][-1])

    real_loader = DataLoader(real_dataset, batch_size=1)

    print(f"total real SRT for unsupervised training: {len(real_loader)}")

    loss_functions = LossFunctions()

    torch.autograd.set_detect_anomaly(True)

    # for i in seed_list:
    for i in seed_list:
        random.seed(i)
        np.random.seed(i)
        torch.manual_seed(i)
        torch.cuda.manual_seed(i)
        torch.backends.cudnn.deterministic = True

        model = GMGATModel(block_type="GCN", num_heads=2, st_encoder_in_channels=[num_hvg, 256, 256],
                           num_classes=domain_classes, num_cell_type=num_cell_types)

        pseudo_dataset = PseudoDataset(data_path=pseudo_st_path, node_num=pseudo_node_num, seed=i, scale=scale)

        pseudo_loader = DataLoader(pseudo_dataset, batch_size=1, shuffle=True)
        print(f"total graphs of pseudo-spots for pre-training: {len(pseudo_loader)}")
        print(f"start training seed{i}")
        train(model, pseudo_loader, real_loader, loss_functions, gpu_id, proj_name, sub_name, num_epochs, i)

        if result:
            import pandas as pd
            if gpu_id != None:
                device = torch.device("cuda:{}".format(gpu_id))
                print(f"Using GPU: {gpu_id}")
            else:
                device = torch.device("cpu")
                print(f"Using CPU")
            model.eval()
            model.to(device)
            for x, ex_adj, sp_adj in real_loader:
                ex_adj, sp_adj = torch.squeeze(ex_adj), torch.squeeze(sp_adj)
                ex_adj = ex_adj.fill_diagonal_(1.)
                sp_adj = sp_adj.fill_diagonal_(1.)
                if gpu_id != None:
                    x, ex_adj, sp_adj = x.to(device), ex_adj.to(device), sp_adj.to(device)

                feature_dict, cls, ratio = model(x, [ex_adj, sp_adj], mode="st")

                ex_adj_out = ratio[0].cpu().detach().numpy()
                ex_adj_out = np.squeeze(ex_adj_out)


                sp_adj_out = ratio[1].cpu().detach().numpy()
                sp_adj_out = np.squeeze(sp_adj_out)

                ex_adj_out = pd.DataFrame(ex_adj_out, columns=header)  # Create a DataFrame with the tensor data and header

                ex_adj_out = ex_adj_out[sorted(ex_adj_out.columns)]

                out_path = f"./outputs/{sub_name}"
                if not os.path.exists(out_path):
                    os.makedirs(out_path)
                    print("Directory created:", out_path)
                else:
                    print("Directory already exists:", out_path)

                model_path = f"./checkpoints/{sub_name}/{sub_name}_{num_epochs}_seed{i}.model"

                ex_adj_out.to_csv(f'{out_path}/ex_adj_out_{model_path.split("/")[-1]}.csv', index=False)  # Save the DataFrame as a CSV file without the index

                sp_adj_out = pd.DataFrame(sp_adj_out, columns=header)  # Create a DataFrame with the tensor data and header

                sp_adj_out = sp_adj_out[sorted(sp_adj_out.columns)]

                sp_adj_out.to_csv(f'{out_path}/sp_adj_out_{model_path.split("/")[-1]}.csv', index=False)  # Save the DataFrame as a CSV file without the index

                mean_out = (ex_adj_out + sp_adj_out) / 2

                mean_out.to_csv(f'{out_path}/mean_out_{model_path.split("/")[-1]}.csv', index=False)

    end_time = time.time()

    duration = end_time - start_time
    print(f"Time taken: {duration:.4f} seconds")
